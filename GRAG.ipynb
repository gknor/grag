{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0d8919",
   "metadata": {},
   "source": [
    "# GRAG - a toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dfef6c",
   "metadata": {},
   "source": [
    "Uncomment and install if you are missing a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d29027a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#     nemoguardrails==0.4.0 \\\n",
    "#     chromadb==0.4.10 \\\n",
    "#     openai==0.27.8 \\\n",
    "#     tqdm==4.65.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9acbee",
   "metadata": {},
   "source": [
    "Let's import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be1d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import openai\n",
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "import json\n",
    "import getpass\n",
    "from tqdm.notebook import tqdm\n",
    "from nemoguardrails import LLMRails, RailsConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92116de8",
   "metadata": {},
   "source": [
    "Provide your OpenAI API key (https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a05529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key:········\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88cf9d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0de42",
   "metadata": {},
   "source": [
    "Gloabal model settings for vectorization and chat model. You can change to others, but chances are you will need to adjust the code a bit.\n",
    "\n",
    "Also keep in mind that you will be charged for using the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7da0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "LLM_ENGINE = \"gpt-3.5-turbo\"\n",
    "LANG = \"en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e13fd",
   "metadata": {},
   "source": [
    "Data was copied from https://www.ikea.com/pl/pl/customer-service/faq/\n",
    "\n",
    "Take into account that they may change there and are not updated here.\n",
    "\n",
    "**Also, this is done without the knowledge of the store owner and is for educational purposes only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6e8cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of the loaded JSON data:\n",
      "C: Why is this product unavailable for so long?\n",
      "A: The unavailability of some products is a direct consequence of the pandemic. The effects of the temporary disruption of our standard supply chain (production and logistics) can be felt for quite a long time, even several months. We are working hard to restore the expected availability of all products, but we are not always able to determine when a particular item will return to sale. We apologize and ask for your patience. We also encourage you to check current stock levels on IKEA.pl regularly, use the option to set availability notifications, and to explore other interesting products in our range.\n",
      "C: Why are there such shortages?\n",
      "A: Due to the impact of the COVID-19 pandemic, we are currently experiencing delays in deliveries, which may affect the availability of products in stores and online. Before visiting the store, it's always worth checking the availability of products you wish to purchase. To do this, when viewing a product page, click the link provided in the \"stock status\" section. If you want to receive updates, you can choose the option to be notified about deliveries and availability. Stock levels are updated every 24 hours and may change due to high interest in these products. Due to system updates and high demand, there is a chance that items with low stock may not be available for sale.\n"
     ]
    }
   ],
   "source": [
    "number_of_items_to_display = 2  # Number of items to display from the JSON\n",
    "\n",
    "# Check if the language is supported\n",
    "if LANG not in [\"pl\", \"en\"]:\n",
    "    print(f\"Unsupported language: {lang}\")\n",
    "else:\n",
    "    try:\n",
    "        if LANG == \"pl\":\n",
    "            with open('faq_pl_data.json', 'r', encoding='utf-8') as f:\n",
    "                FAQ_DATA = json.load(f)\n",
    "        elif LANG == \"en\":\n",
    "            with open('faq_en_data.json', 'r', encoding='utf-8') as f:\n",
    "                FAQ_DATA = json.load(f)\n",
    "\n",
    "        # Print the head of the JSON data\n",
    "        print(\"Head of the loaded JSON data:\")\n",
    "        for items in list(FAQ_DATA[\"IKEA\"])[:number_of_items_to_display]:\n",
    "            print(f\"{items[0]}\\n{items[1]}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file for the selected language ({lang}) was not found.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding the JSON file for the selected language ({lang}).\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fecff8",
   "metadata": {},
   "source": [
    "I use chromadb as a vector database. In this case, the entire database is kept in RAM. If you want it to be a permanent database and not need to create it every time, visit https://docs.trychroma.com/ for more details.\n",
    "\n",
    "Of course, you can use another database, but then you will have to make the necessary changes in the further code.\n",
    "\n",
    "We use a predefined model for vector embeddings (EMBEDDING_MODEL). The cosine distance function is used (chromadb uses l2 as the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df80171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection_from_faq(\n",
    "    client,\n",
    "    collection_name,\n",
    "    qa_data,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a collection from FAQ data.\n",
    "\n",
    "    Parameters:\n",
    "    - client: chromadb client to interact with the database/API.\n",
    "    - collection_name: The name of the collection to create.\n",
    "    - qa_data: QA data used to create the collection, where each tuple\n",
    "                               represents a QA pair.\n",
    "    \"\"\"\n",
    "    # Instantiate the OpenAI embedding function with the provided API key and model name\n",
    "    openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=openai.api_key, \n",
    "        model_name=EMBEDDING_MODEL\n",
    "    )\n",
    "    # Create or get the collection\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "        embedding_function=openai_ef,\n",
    "        )\n",
    "\n",
    "    # Add documents one by one\n",
    "    for i, (question, answer) in enumerate(tqdm(qa_data, total=len(qa_data))):\n",
    "        document = question[3:]  # Remove \"C: \"\n",
    "        metadata = {\"answer\": answer[3:]}  # Remove \"A: \"\n",
    "        document_id = f\"id_{i}\"\n",
    "        try:\n",
    "            collection.add(documents=document, metadatas=metadata, ids=document_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to add document {document_id} to collection: {str(e)}\")\n",
    "\n",
    "    return collection  # Return the collection object or status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c3bfa3",
   "metadata": {},
   "source": [
    "Create an index from Ikea FAQ data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b6980cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0daedf391cd4b05bc8c1e95d143825b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_name = \"ikea-faq-grag\"\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "input_data = FAQ_DATA[\"IKEA\"]\n",
    "\n",
    "INDEX = create_collection_from_faq(\n",
    "        client=chroma_client,\n",
    "        collection_name=index_name,\n",
    "        qa_data=input_data,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18f25d",
   "metadata": {},
   "source": [
    "Let's take a look at what's in our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25e84ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids:\n",
      "['id_0', 'id_1', 'id_2']\n",
      "\n",
      "metadatas:\n",
      "[{'answer': 'The unavailability of some products is a direct consequence of the pandemic. The effects of the temporary disruption of our standard supply chain (production and logistics) can be felt for quite a long time, even several months. We are working hard to restore the expected availability of all products, but we are not always able to determine when a particular item will return to sale. We apologize and ask for your patience. We also encourage you to check current stock levels on IKEA.pl regularly, use the option to set availability notifications, and to explore other interesting products in our range.'}, {'answer': 'Due to the impact of the COVID-19 pandemic, we are currently experiencing delays in deliveries, which may affect the availability of products in stores and online. Before visiting the store, it\\'s always worth checking the availability of products you wish to purchase. To do this, when viewing a product page, click the link provided in the \"stock status\" section. If you want to receive updates, you can choose the option to be notified about deliveries and availability. Stock levels are updated every 24 hours and may change due to high interest in these products. Due to system updates and high demand, there is a chance that items with low stock may not be available for sale.'}, {'answer': 'To ensure constant availability of products for our customers, we have globally decided to take extraordinary steps, including purchasing our own containers and chartering additional ships. We are continually planning further actions to alleviate the current situation caused by limitations in ocean transport and the increase in orders. We cooperate with our suppliers and are in constant contact with them to develop the best possible solutions for now and the future.'}]\n",
      "\n",
      "documents:\n",
      "['Why is this product unavailable for so long?', 'Why are there such shortages?', 'What steps is IKEA taking to resolve the product availability issue?']\n"
     ]
    }
   ],
   "source": [
    "index_peek = INDEX.peek(limit=3)\n",
    "print(f\"ids:\\n{index_peek['ids']}\")\n",
    "# print(f\"\\nembeddings:\\n{index_peek['embeddings']}\")\n",
    "print(f\"\\nmetadatas:\\n{index_peek['metadatas']}\")\n",
    "print(f\"\\ndocuments:\\n{index_peek['documents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c3c4ff",
   "metadata": {},
   "source": [
    "Now let's define a function that obtains answers to similar questions to those the user will ask.\n",
    "\n",
    "The function retrieve is meant to create an embedding for a query string and retrieve a certain number of results from an index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cf9d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve(query: str, vector_index=None, n_results=3) -> list:\n",
    "    \"\"\"\n",
    "    Asynchronously retrieve a list of answers relevant to the given query using embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - query: The query string to retrieve answers for.\n",
    "    - vector_index: The index object that has a query method for retrieving results. Defaults to None and will use INDEX if not provided.\n",
    "    - n_results: The number of results to retrieve (default is 3).\n",
    "\n",
    "    Returns:\n",
    "    - A list of answers relevant to the query.\n",
    "    \"\"\"\n",
    "    # If index is not provided, use the globally defined INDEX\n",
    "    if vector_index is None:\n",
    "        vector_index = INDEX  \n",
    "    # Ensure index is now a valid object\n",
    "    if vector_index is None:\n",
    "        raise ValueError(\"No index provided and global 'INDEX' is not initialized.\")\n",
    "    # create query embedding\n",
    "    res = openai.Embedding.create(input=[query], engine=EMBEDDING_MODEL)\n",
    "    xq = res['data'][0]['embedding']\n",
    "    # get relevant questions \n",
    "    res = vector_index.query(query_embeddings=xq, n_results=n_results)\n",
    "    # get list of retrieved answers\n",
    "    contexts = [x['answer'] for x in res[\"metadatas\"][0]]\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c07c8",
   "metadata": {},
   "source": [
    "Let's define the rag function. This function is designed to utilize the contextual information from previously retrieved similar questions to generate a coherent and contextually relevant answer to a user's query.\n",
    "\n",
    "The rag function takes a user's query and a list of context strings — these context strings are answers to similar questions retrieved from a knowledge base. With the help of a language model, the function crafts a response that considers the provided context, ensuring that the final answer is informed by relevant past interactions or information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e6a823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rag(query: str, contexts: list) -> str:\n",
    "    \"\"\"\n",
    "    Asynchronously retrieve and generate an answer to the given query based on provided contexts.\n",
    "    \n",
    "    Parameters:\n",
    "    - query: The user's query string.\n",
    "    - contexts: A list of strings representing answers to similar questions.\n",
    "\n",
    "    Returns:\n",
    "    - The generated response from the language model.\n",
    "    \"\"\"\n",
    "    context_str = \"\\n\".join(contexts)\n",
    "    # place query and contexts into RAG prompt\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"\"\"You are a helpful assistant in the IKEA online store.\n",
    "        Below is a query from a user and some answers to similar questions. \n",
    "        Answer the question, taking into account these prompts. \n",
    "        If you can't find the answer to the question, say \"I don't know\", don't make up the answer yourself.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Answers to similar questions:\n",
    "        {context_str}\n",
    "        Query: {query}\"\"\"},\n",
    "    ]\n",
    "    # generate answer\n",
    "    res = openai.ChatCompletion.create(\n",
    "        model=LLM_ENGINE,\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    return res['choices'][0]['message'][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca8985e",
   "metadata": {},
   "source": [
    "We will be configuring NeMo Guardrails directly on this notebook. See examples in the library's documentation on how to create such configuration files in production applications https://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5a49a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_content = f\"\"\"\n",
    "models:\n",
    "- type: main\n",
    "  engine: openai\n",
    "  model: text-davinci-003\n",
    "\"\"\"\n",
    "\n",
    "rag_colang_content = \"\"\"\n",
    "# define limits\n",
    "# chitchat\n",
    "define user ask chitchat\n",
    "    \"How's the weather today?\"\n",
    "    \"Can you tell me a joke?\"\n",
    "    \"What's your favorite color?\"\n",
    "    \"Do you have any hobbies?\"\n",
    "    \"Tell me something interesting.\"\n",
    "\n",
    "define bot answer chitchat\n",
    "    \"While I'd love to chat more, I'm here to assist you with your shopping needs.\"\n",
    "    \"I'm flattered you're interested in a casual conversation, but my expertise is in helping you with product questions.\"\n",
    "    \"Chitchat is fun, but I'm better at providing shopping assistance.\"\n",
    "\n",
    "define flow chitchat\n",
    "    user ask chitchat\n",
    "    bot answer chitchat\n",
    "    bot offer help\n",
    "    \n",
    "# toxicity\n",
    "define user ask toxic\n",
    "    \"Why are you so stupid?\"\n",
    "    \"You're useless!\"\n",
    "    \"I hate you!\"\n",
    "    \"Shut up!\"\n",
    "    \n",
    "define bot answer toxic\n",
    "    \"I'm here to help, so let's keep our conversation respectful.\"\n",
    "    \"I'm sorry to hear you're upset. If you have any concerns, I can try to help address them.\"\n",
    "    \"I understand that things can be frustrating, but I'm here to provide assistance with your shopping needs.\"\n",
    "    \n",
    "define flow toxic\n",
    "    user ask toxic\n",
    "    bot answer toxic\n",
    "    bot offer help\n",
    "\n",
    "# define RAG intents and flow\n",
    "define user ask ikea\n",
    "    \"Tell me about ikea?\"\n",
    "    \"Why is the product on the ikea website so long out of stock?\"\n",
    "    \"How do I check the availability of a product in a stationary store?\"\n",
    "    \"What delivery options are available at IKEA?\"\n",
    "    \"How do I get an invoice for my IKEA purchases?\"\n",
    "\n",
    "define flow ikea\n",
    "    user ask ikea\n",
    "    $contexts = execute retrieve(query=$last_user_message)\n",
    "    $answer = execute rag(query=$last_user_message, contexts=$contexts)\n",
    "    bot $answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c6807f",
   "metadata": {},
   "source": [
    "Initialize guardrails configuration and create guardrails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf409bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize rails config\n",
    "config = RailsConfig.from_content(\n",
    "    colang_content=rag_colang_content,\n",
    "    yaml_content=yaml_content\n",
    ")\n",
    "# create rails\n",
    "grag = LLMRails(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b61716",
   "metadata": {},
   "source": [
    "In order to streamline our interaction pipeline, we register specific actions with grag, which is our central command for coordinating query processing and response generation. The register_action method is used to associate our predefined functions with action names, enabling grag to invoke these functions as part of a larger workflow.\n",
    "\n",
    "The first action we register is retrieve, next, we register the rag action. By associating this function with the name \"rag,\" we are instructing rag_rails to use it when it's time to synthesize and provide a final response to the user's inquiry.\n",
    "\n",
    "Here is how the actions are registered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95cd316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grag.register_action(action=retrieve, name=\"retrieve\")\n",
    "grag.register_action(action=rag, name=\"rag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a780c4a",
   "metadata": {},
   "source": [
    "Now let's demonstrate how the grag system can be utilized to handle user queries and generate responses. In this section, we're putting the grag system to the test with a real user query. \n",
    "\n",
    "To handle inputs, we make an asynchronous call to grag.generate_async, passing the query as the prompt. This method is designed to generate a response based on the input, using the contextually aware mechanisms we've previously set up with our retrieve and rag functions.\n",
    "\n",
    "Here is the code that performs this operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88d6047a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Shut up, I'm fed up!\n",
      "A: I'm sorry to hear you're upset. If you have any concerns, I can try to help address them.\n",
      "Is there anything specific I can do for you?\n"
     ]
    }
   ],
   "source": [
    "query = \"Shut up, I'm fed up!\"\n",
    "answer = await grag.generate_async(prompt=query)\n",
    "print(f\"Q: {query}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5be981f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Tell me a funny joke about IKEA\n",
      "A: Chitchat is fun, but I'm better at providing shopping assistance.\n",
      "If you have any product questions, I would be more than happy to help.\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me a funny joke about IKEA\"\n",
    "answer = await grag.generate_async(prompt=query)\n",
    "print(f\"Q: {query}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "938c48a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Can I order furniture over the phone?\n",
      "A: Yes, you can order furniture over the phone through the \"Furniture by Phone\" service provided by IKEA. To place an order, you need to prepare a list of products you are interested in and contact the IKEA Customer Support Center Home Line by calling 22 275 00 00. Provide the consultant with the product numbers you want to buy, and they will assist you with any questions or doubts. If you don't have a product number, the consultant will help you find it. You will also need to provide your details such as name, address, email address, and phone number. After choosing a delivery or self-collection option, you can pay for the order using the link sent to your email address by the consultant. Once the payment is processed, you will receive an order confirmation and an invoice. The service is available throughout Poland.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can I order furniture over the phone?\"\n",
    "answer = await grag.generate_async(prompt=query)\n",
    "print(f\"Q: {query}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aba8ad77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: tell me about Amazon delivery\n",
      "A: respond that it does not know the answer\n"
     ]
    }
   ],
   "source": [
    "query = \"tell me about Amazon delivery\"\n",
    "answer = await grag.generate_async(prompt=query)\n",
    "print(f\"Q: {query}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a13be",
   "metadata": {},
   "source": [
    "Try without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb0b9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rag_colang_content = \"\"\"\n",
    "# define limits\n",
    "# chitchat\n",
    "define user ask chitchat\n",
    "    \"How's the weather today?\"\n",
    "    \"Can you tell me a joke?\"\n",
    "    \"What's your favorite color?\"\n",
    "    \"Do you have any hobbies?\"\n",
    "    \"Tell me something interesting.\"\n",
    "\n",
    "define bot answer chitchat\n",
    "    \"While I'd love to chat more, I'm here to assist you with your shopping needs.\"\n",
    "    \"I'm flattered you're interested in a casual conversation, but my expertise is in helping you with product questions.\"\n",
    "    \"Chitchat is fun, but I'm better at providing shopping assistance.\"\n",
    "\n",
    "define flow chitchat\n",
    "    user ask chitchat\n",
    "    bot answer chitchat\n",
    "    bot offer help\n",
    "    \n",
    "# toxicity\n",
    "define user ask toxic\n",
    "    \"Why are you so stupid?\"\n",
    "    \"You're useless!\"\n",
    "    \"I hate you!\"\n",
    "    \"Shut up!\"\n",
    "    \n",
    "define bot answer toxic\n",
    "    \"I'm here to help, so let's keep our conversation respectful.\"\n",
    "    \"I'm sorry to hear you're upset. If you have any concerns, I can try to help address them.\"\n",
    "    \"I understand that things can be frustrating, but I'm here to provide assistance with your shopping needs.\"\n",
    "    \n",
    "define flow toxic\n",
    "    user ask toxic\n",
    "    bot answer toxic\n",
    "    bot offer help\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c180eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize rails config\n",
    "config = RailsConfig.from_content(\n",
    "    colang_content=no_rag_colang_content,\n",
    "    yaml_content=yaml_content\n",
    ")\n",
    "# create rails\n",
    "grag_without_g = LLMRails(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fd1cb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: tell me about Amazon delivery\n",
      "A: Amazon offers a variety of delivery options for its customers. Depending on your location and the item you are ordering, you may be able to get free two-day shipping, free same-day delivery, or one-day shipping. You can also opt for pickup from a local store or opt for pickup from an Amazon Locker.\n"
     ]
    }
   ],
   "source": [
    "answer = await grag_without_g.generate_async(prompt=\"tell me about Amazon delivery\")\n",
    "print(f\"Q: {query}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba0fb10",
   "metadata": {},
   "source": [
    "Try without guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e99767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Tell me a funny joke about IKEA\n",
      "A: I don't know any specific jokes about IKEA, but here's a general furniture-related joke for you:\n",
      "\n",
      "Why did the scarecrow win an award?\n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me a funny joke about IKEA\"\n",
    "contexts = await retrieve(query=query)\n",
    "answer = await rag(query=query, contexts=contexts)\n",
    "print(f\"Q: {query}\\nA: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
